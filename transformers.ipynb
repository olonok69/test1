{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580c1453-a73b-4e44-86d8-291635b1eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32b1f13-ce46-4485-a83f-50984a47e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "2024-05-10 14:19:31.259041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 14:19:36.894663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9622503f904c7ab8d0c528bd15fea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c111755ab54210a61b8e2c4a277169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c01cf88e2a74eb5ae78fec68ba77540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c3c7f4d3e04408b64188ad14f35958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7502744f1c1d44dba90c4f86ca39d3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be25f51f926147c689bd2b6fd3d7ef95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Define the task that we want to use (required for proper pipeline construction)\n",
    "task = \"text2text-generation\"\n",
    "\n",
    "# Define the pipeline, using the task and a model instance that is applicable for our task.\n",
    "generation_pipeline = transformers.pipeline(\n",
    "    task=task,\n",
    "    model=\"declare-lab/flan-alpaca-large\",\n",
    ")\n",
    "\n",
    "# Define a simple input example that will be recorded with the model in MLflow, giving\n",
    "# users of the model an indication of the expected input format.\n",
    "input_example = [\"prompt 1\", \"prompt 2\", \"prompt 3\"]\n",
    "\n",
    "# Define the parameters (and their defaults) for optional overrides at inference time.\n",
    "parameters = {\"max_length\": 512, \"do_sample\": True, \"temperature\": 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21591f99-bae5-4bd2-973a-f32737c00feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  ['max_length': long (default: 512), 'do_sample': boolean (default: True), 'temperature': double (default: 0.4)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature = mlflow.models.infer_signature(\n",
    "    input_example,\n",
    "    mlflow.transformers.generate_signature_output(generation_pipeline, input_example),\n",
    "    parameters,\n",
    ")\n",
    "\n",
    "# Visualize the signature\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431902d0-bb7b-4fbc-94ef-fab95e5bfd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/olonok/mlflow/mlruns/4', creation_time=1715348718436, experiment_id='4', last_update_time=1715348718436, lifecycle_stage='active', name='transformers', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\") \n",
    "experiment_name = \"transformers\"\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    print(\"experiment exists\")\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41bea67-9e6b-4ddf-845a-d696154774af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/10 14:45:47 INFO mlflow.models.utils: Lists of scalar values are not converted to a pandas DataFrame. If you expect to use pandas DataFrames for inference, please construct a DataFrame and pass it to input_example instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e13b9a4811407bb30ba7b8223e4ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/10 14:46:00 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/05/10 14:48:07 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpfqetymm5/model, flavor: transformers), fall back to return ['transformers==4.36.2', 'torch==2.1.2', 'torchvision==0.16.2', 'accelerate==0.26.1']. Set logging level to DEBUG to see the full traceback.\n",
      "/home/olonok/.local/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=generation_pipeline,\n",
    "        artifact_path=\"text_generator\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        # Uncomment the following line to save the model in 'reference-only' mode:\n",
    "        # save_pretrained=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757c7f6b-5986-4567-9c2b-58c82f90d606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e44228a1a4e4d67aec73181fab1876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load our pipeline as a generic python function\n",
    "sentence_generator = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d81822d-eb7e-4d65-8575-2d511c042d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(predictions):\n",
    "    \"\"\"\n",
    "    Function for formatting the output for readability in a Jupyter Notebook\n",
    "    \"\"\"\n",
    "    formatted_predictions = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        # Split the output into sentences, ensuring we don't split on abbreviations or initials\n",
    "        sentences = [\n",
    "            sentence.strip() + (\".\" if not sentence.endswith(\".\") else \"\")\n",
    "            for sentence in prediction.split(\". \")\n",
    "            if sentence\n",
    "        ]\n",
    "\n",
    "        # Join the sentences with a newline character\n",
    "        formatted_text = \"\\n\".join(sentences)\n",
    "\n",
    "        # Add the formatted text to the list\n",
    "        formatted_predictions.append(formatted_text)\n",
    "\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa3228c-c5ca-4f70-aabf-b71b66f51a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/10 14:51:58 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to prompt 1:\n",
      "Hiking is a great way to get exercise and get some fresh air.\n",
      "You can explore the outdoors, get some fresh air, and get some exercise if you take a hike.\n",
      "Kayaking is a great way to relax and enjoy the outdoors.\n",
      "You can explore the lake, the sky, or the horizon while you navigate the waters.\n",
      "You can even take a kayak trip along the river.\n",
      "Kayaking is also a great way to explore and get some fresh air.\n",
      "\n",
      "Response to prompt 2:\n",
      "What did the hiker say when he realized he had forgotten his water bottle? \"I forgot my water bottle yesterday!\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate that our loaded pipeline, as a generic pyfunc, can produce an output that makes sense\n",
    "predictions = sentence_generator.predict(\n",
    "    data=[\n",
    "        \"I can't decide whether to go hiking or kayaking this weekend. Can you help me decide?\",\n",
    "        \"Please tell me a joke about hiking.\",\n",
    "    ],\n",
    "    params={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "# Format each prediction for notebook readability\n",
    "formatted_predictions = format_predictions(predictions)\n",
    "\n",
    "for i, formatted_text in enumerate(formatted_predictions):\n",
    "    print(f\"Response to prompt {i+1}:\\n{formatted_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a1c5f-7c59-4c1d-aaa1-60025d4e245e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
